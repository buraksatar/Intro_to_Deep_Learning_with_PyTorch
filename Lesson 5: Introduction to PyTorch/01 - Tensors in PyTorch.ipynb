{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors, for a Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n"
    }
   ],
   "source": [
    "### Generate some data\n",
    "\n",
    "# Set the random seed, so things are predictable\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "\n",
    "# True weights for our data,\n",
    "# random normal variables again\n",
    "weigths = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.rand((1, 1))\n",
    "print(weigths)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`features = torch.randn((1, 5))` creates a tensor with shape `(1, 5)`, one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one. \n",
    "\n",
    "`weights = torch.randn_like(features)` creates another tensor with the same shape as `features`, again containing values from a normal distribution.\n",
    "\n",
    "Finally, `bias = torch.randn((1, 1))` creates a single value from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[0.2504]])\n"
    }
   ],
   "source": [
    "## Calculate the output of this network using\n",
    "## the weights and bias tensors\n",
    "output = activation(\n",
    "        torch.sum(features * weigths) + bias\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.\n",
    "\n",
    "But we can face with dimension problems.\n",
    "\n",
    "There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "\n",
    "I usually use `.view()`, but any of the three methods will work for this. So, now we can reshape `weights` to have five rows and one column with something like `weights.view(5, 1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm is more strict about tensor shape. but torch.matmul do broadcasting a sometimes gives useless data. so always use torch.mm to get what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[0.2504]])\n"
    }
   ],
   "source": [
    "## Calculate the output of this network using\n",
    "## the matrix multiplication\n",
    "output = activation(\n",
    "        torch.mm(features, weigths.view(5, 1)) + bias\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Them up - Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(7)\n",
    "\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "# No of input units, must match no of input features\n",
    "n_input = features.shape[1]\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "\n",
    "# Weights for inputs  to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the output for this multi-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[0.3171]])\n"
    }
   ],
   "source": [
    "hidden_output = activation(torch.mm( features, W1) + B1)\n",
    "\n",
    "output = activation( torch.mm(hidden_output, W2) + B2)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy to Torch an back\n",
    "\n",
    "PyTorch has a great feature for converting between Numpy arrays and Torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.21654698,  0.29338615,  0.92256296],\n       [ 0.6410212 ,  0.10907066,  0.91496208],\n       [ 0.19697447,  0.35923486,  0.391553  ],\n       [ 0.55148733,  0.76812176,  0.51890214]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2165, 0.2934, 0.9226],\n        [0.6410, 0.1091, 0.9150],\n        [0.1970, 0.3592, 0.3916],\n        [0.5515, 0.7681, 0.5189]], dtype=torch.float64)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.21654698,  0.29338615,  0.92256296],\n       [ 0.6410212 ,  0.10907066,  0.91496208],\n       [ 0.19697447,  0.35923486,  0.391553  ],\n       [ 0.55148733,  0.76812176,  0.51890214]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if we change the values **in-place** of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4331, 0.5868, 1.8451],\n        [1.2820, 0.2181, 1.8299],\n        [0.3939, 0.7185, 0.7831],\n        [1.1030, 1.5362, 1.0378]], dtype=torch.float64)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.43309395,  0.58677229,  1.84512592],\n       [ 1.2820424 ,  0.21814132,  1.82992416],\n       [ 0.39394894,  0.71846972,  0.783106  ],\n       [ 1.10297465,  1.53624351,  1.03780427]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('Intro2PyTorch': conda)",
   "language": "python",
   "name": "python361064bitintro2pytorchcondae9211d0515cf4928a057cbee7e6d59e9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}