{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We create a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create 2 dictionaries\n",
    "# int2char\n",
    "# char2int\n",
    "chars = tuple(set(text))\n",
    "#print(chars)\n",
    "int2char = dict(enumerate(chars))\n",
    "#print(int2char)\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "#print(char2int)\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 37, 77,  8, 57, 56, 25, 59,  7, 60, 60, 60, 21, 77,  8,  8, 62,\n",
       "       59, 63, 77, 19, 54,  2, 54, 56, 27, 59, 77, 25, 56, 59, 77,  2,  2,\n",
       "       59, 77,  2, 54, 58, 56, 71, 59, 56, 13, 56, 25, 62, 59, 45, 75, 37,\n",
       "       77,  8,  8, 62, 59, 63, 77, 19, 54,  2, 62, 59, 54, 27, 59, 45, 75,\n",
       "       37, 77,  8,  8, 62, 59, 54, 75, 59, 54, 57, 27, 59, 29, 47, 75, 60,\n",
       "       47, 77, 62, 49, 60, 60, 39, 13, 56, 25, 62, 57, 37, 54, 75])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data\n",
    "\n",
    "But, our LSTM expects an input that is one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # initialize the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    #print(one_hot)\n",
    "    # finally reshape it to get backthe original way\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 10)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr) // batch_size_total\n",
    "    #print(n_batches)\n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches*batch_size_total]\n",
    "    #print(len(arr))\n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    #print(arr.shape)\n",
    "\n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        #print(x)\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        #print(y)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[37 77  8  8 62 59 63 77 19 54]\n",
      " [78  3 37 67 59 19 56 25 61 62]\n",
      " [75 33 59 57 29 47 77 25 33 27]\n",
      " [59 47 37 54 61 37 59 75 29 75]\n",
      " [23  2 56 59 47 54 57 37 59 27]\n",
      " [56 59 57 29 59 37 54 19 49 59]\n",
      " [27 59 25 29 29 19 67 60 63 29]\n",
      " [56 59 57 77 58 56 75 59 45  8]]\n",
      "\n",
      "y\n",
      " [[77  8  8 62 59 63 77 19 54  2]\n",
      " [ 3 37 67 59 19 56 25 61 62 64]\n",
      " [33 59 57 29 47 77 25 33 27 59]\n",
      " [47 37 54 61 37 59 75 29 75 56]\n",
      " [ 2 56 59 47 54 57 37 59 27 45]\n",
      " [59 57 29 59 37 54 19 49 59 21]\n",
      " [59 25 29 29 19 67 60 63 29 25]\n",
      " [59 57 77 58 56 75 59 45  8 59]]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(batches)\n",
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network\n",
    "\n",
    "## Model Structure\n",
    "\n",
    "In `__init__`:\n",
    "- Create and store the dictonaries\n",
    "- Define an LSTN layer that takes as params: an input size(number of characters), a hidden layer size `n_hidden`, a number of layers `n_layers`, a dropout prob, and a batch_first boolean\n",
    "- define a dropout layer,\n",
    "- define FC with params: input size `n_hidden` and output size(number of characters)\n",
    "- Finally initialize the weights\n",
    "\n",
    "`input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell.\n",
    "\n",
    "Finally, in the `forward` function, we can stack up the LSTM cells into layers using `.view`. With this, you pass in a list of cells and it will send the output of one cell into the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, \n",
    "                 drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # dict\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # layers of model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # get the outpus and the new hidden state from the lstm\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # reshape\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Train\n",
    "\n",
    "A couple of details about training: \n",
    ">* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new *tuple* variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
    "* We use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, \n",
    "          lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # initilaze hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            # one hot encode our data and make them torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated grads\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                        \"Step: {}...\".format(counter),\n",
    "                        \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                        \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=32, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set your model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden=32\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 10... Loss: 4.3389... Val Loss: 4.3298\n",
      "Epoch: 1/3... Step: 20... Loss: 4.1698... Val Loss: 4.1358\n",
      "Epoch: 1/3... Step: 30... Loss: 3.7976... Val Loss: 3.7027\n",
      "Epoch: 1/3... Step: 40... Loss: 3.5007... Val Loss: 3.3857\n",
      "Epoch: 1/3... Step: 50... Loss: 3.3485... Val Loss: 3.2254\n",
      "Epoch: 1/3... Step: 60... Loss: 3.2835... Val Loss: 3.1712\n",
      "Epoch: 1/3... Step: 70... Loss: 3.2909... Val Loss: 3.1530\n",
      "Epoch: 1/3... Step: 80... Loss: 3.2285... Val Loss: 3.1444\n",
      "Epoch: 1/3... Step: 90... Loss: 3.2568... Val Loss: 3.1402\n",
      "Epoch: 1/3... Step: 100... Loss: 3.2261... Val Loss: 3.1374\n",
      "Epoch: 1/3... Step: 110... Loss: 3.2291... Val Loss: 3.1352\n",
      "Epoch: 1/3... Step: 120... Loss: 3.2116... Val Loss: 3.1332\n",
      "Epoch: 1/3... Step: 130... Loss: 3.1998... Val Loss: 3.1322\n",
      "Epoch: 1/3... Step: 140... Loss: 3.2135... Val Loss: 3.1303\n",
      "Epoch: 1/3... Step: 150... Loss: 3.2184... Val Loss: 3.1289\n",
      "Epoch: 1/3... Step: 160... Loss: 3.1864... Val Loss: 3.1282\n",
      "Epoch: 1/3... Step: 170... Loss: 3.1768... Val Loss: 3.1268\n",
      "Epoch: 1/3... Step: 180... Loss: 3.1610... Val Loss: 3.1245\n",
      "Epoch: 1/3... Step: 190... Loss: 3.1902... Val Loss: 3.1226\n",
      "Epoch: 1/3... Step: 200... Loss: 3.1533... Val Loss: 3.1207\n",
      "Epoch: 1/3... Step: 210... Loss: 3.1487... Val Loss: 3.1181\n",
      "Epoch: 1/3... Step: 220... Loss: 3.1317... Val Loss: 3.1162\n",
      "Epoch: 1/3... Step: 230... Loss: 3.1673... Val Loss: 3.1141\n",
      "Epoch: 1/3... Step: 240... Loss: 3.1700... Val Loss: 3.1113\n",
      "Epoch: 1/3... Step: 250... Loss: 3.1455... Val Loss: 3.1072\n",
      "Epoch: 1/3... Step: 260... Loss: 3.1202... Val Loss: 3.1028\n",
      "Epoch: 1/3... Step: 270... Loss: 3.1323... Val Loss: 3.0987\n",
      "Epoch: 1/3... Step: 280... Loss: 3.0920... Val Loss: 3.0938\n",
      "Epoch: 1/3... Step: 290... Loss: 3.1083... Val Loss: 3.0875\n",
      "Epoch: 1/3... Step: 300... Loss: 3.0745... Val Loss: 3.0805\n",
      "Epoch: 1/3... Step: 310... Loss: 3.1008... Val Loss: 3.0727\n",
      "Epoch: 1/3... Step: 320... Loss: 3.0985... Val Loss: 3.0649\n",
      "Epoch: 1/3... Step: 330... Loss: 3.0909... Val Loss: 3.0566\n",
      "Epoch: 1/3... Step: 340... Loss: 3.0654... Val Loss: 3.0474\n",
      "Epoch: 1/3... Step: 350... Loss: 3.0129... Val Loss: 3.0374\n",
      "Epoch: 1/3... Step: 360... Loss: 3.0299... Val Loss: 3.0274\n",
      "Epoch: 1/3... Step: 370... Loss: 3.0313... Val Loss: 3.0174\n",
      "Epoch: 1/3... Step: 380... Loss: 3.0152... Val Loss: 3.0076\n",
      "Epoch: 1/3... Step: 390... Loss: 3.0182... Val Loss: 2.9980\n",
      "Epoch: 1/3... Step: 400... Loss: 3.0374... Val Loss: 2.9881\n",
      "Epoch: 1/3... Step: 410... Loss: 2.9609... Val Loss: 2.9785\n",
      "Epoch: 1/3... Step: 420... Loss: 2.9893... Val Loss: 2.9684\n",
      "Epoch: 1/3... Step: 430... Loss: 2.9988... Val Loss: 2.9588\n",
      "Epoch: 1/3... Step: 440... Loss: 3.0141... Val Loss: 2.9510\n",
      "Epoch: 1/3... Step: 450... Loss: 2.9979... Val Loss: 2.9415\n",
      "Epoch: 1/3... Step: 460... Loss: 2.9570... Val Loss: 2.9328\n",
      "Epoch: 1/3... Step: 470... Loss: 2.9548... Val Loss: 2.9248\n",
      "Epoch: 1/3... Step: 480... Loss: 2.9673... Val Loss: 2.9172\n",
      "Epoch: 1/3... Step: 490... Loss: 2.9236... Val Loss: 2.9120\n",
      "Epoch: 1/3... Step: 500... Loss: 2.9204... Val Loss: 2.9035\n",
      "Epoch: 1/3... Step: 510... Loss: 2.9599... Val Loss: 2.8949\n",
      "Epoch: 1/3... Step: 520... Loss: 2.9320... Val Loss: 2.8891\n",
      "Epoch: 1/3... Step: 530... Loss: 2.9195... Val Loss: 2.8827\n",
      "Epoch: 1/3... Step: 540... Loss: 2.8917... Val Loss: 2.8760\n",
      "Epoch: 1/3... Step: 550... Loss: 2.9276... Val Loss: 2.8688\n",
      "Epoch: 2/3... Step: 560... Loss: 2.9042... Val Loss: 2.8631\n",
      "Epoch: 2/3... Step: 570... Loss: 2.8894... Val Loss: 2.8589\n",
      "Epoch: 2/3... Step: 580... Loss: 2.9088... Val Loss: 2.8518\n",
      "Epoch: 2/3... Step: 590... Loss: 2.9316... Val Loss: 2.8459\n",
      "Epoch: 2/3... Step: 600... Loss: 2.9257... Val Loss: 2.8400\n",
      "Epoch: 2/3... Step: 610... Loss: 2.8647... Val Loss: 2.8423\n",
      "Epoch: 2/3... Step: 620... Loss: 2.8726... Val Loss: 2.8315\n",
      "Epoch: 2/3... Step: 630... Loss: 2.8691... Val Loss: 2.8244\n",
      "Epoch: 2/3... Step: 640... Loss: 2.8691... Val Loss: 2.8183\n",
      "Epoch: 2/3... Step: 650... Loss: 2.8733... Val Loss: 2.8127\n",
      "Epoch: 2/3... Step: 660... Loss: 2.8424... Val Loss: 2.8084\n",
      "Epoch: 2/3... Step: 670... Loss: 2.8599... Val Loss: 2.8059\n",
      "Epoch: 2/3... Step: 680... Loss: 2.8529... Val Loss: 2.8023\n",
      "Epoch: 2/3... Step: 690... Loss: 2.8188... Val Loss: 2.7966\n",
      "Epoch: 2/3... Step: 700... Loss: 2.8567... Val Loss: 2.7877\n",
      "Epoch: 2/3... Step: 710... Loss: 2.8864... Val Loss: 2.7822\n",
      "Epoch: 2/3... Step: 720... Loss: 2.8372... Val Loss: 2.7794\n",
      "Epoch: 2/3... Step: 730... Loss: 2.8275... Val Loss: 2.7743\n",
      "Epoch: 2/3... Step: 740... Loss: 2.7935... Val Loss: 2.7663\n",
      "Epoch: 2/3... Step: 750... Loss: 2.8336... Val Loss: 2.7622\n",
      "Epoch: 2/3... Step: 760... Loss: 2.8152... Val Loss: 2.7568\n",
      "Epoch: 2/3... Step: 770... Loss: 2.8005... Val Loss: 2.7495\n",
      "Epoch: 2/3... Step: 780... Loss: 2.7823... Val Loss: 2.7445\n",
      "Epoch: 2/3... Step: 790... Loss: 2.8147... Val Loss: 2.7398\n",
      "Epoch: 2/3... Step: 800... Loss: 2.8291... Val Loss: 2.7353\n",
      "Epoch: 2/3... Step: 810... Loss: 2.7800... Val Loss: 2.7315\n",
      "Epoch: 2/3... Step: 820... Loss: 2.7584... Val Loss: 2.7235\n",
      "Epoch: 2/3... Step: 830... Loss: 2.7853... Val Loss: 2.7197\n",
      "Epoch: 2/3... Step: 840... Loss: 2.7902... Val Loss: 2.7161\n",
      "Epoch: 2/3... Step: 850... Loss: 2.7468... Val Loss: 2.7120\n",
      "Epoch: 2/3... Step: 860... Loss: 2.7561... Val Loss: 2.7036\n",
      "Epoch: 2/3... Step: 870... Loss: 2.7545... Val Loss: 2.6991\n",
      "Epoch: 2/3... Step: 880... Loss: 2.7367... Val Loss: 2.6941\n",
      "Epoch: 2/3... Step: 890... Loss: 2.7797... Val Loss: 2.6898\n",
      "Epoch: 2/3... Step: 900... Loss: 2.7784... Val Loss: 2.6848\n",
      "Epoch: 2/3... Step: 910... Loss: 2.7390... Val Loss: 2.6806\n",
      "Epoch: 2/3... Step: 920... Loss: 2.7525... Val Loss: 2.6739\n",
      "Epoch: 2/3... Step: 930... Loss: 2.7220... Val Loss: 2.6685\n",
      "Epoch: 2/3... Step: 940... Loss: 2.7414... Val Loss: 2.6663\n",
      "Epoch: 2/3... Step: 950... Loss: 2.7347... Val Loss: 2.6577\n",
      "Epoch: 2/3... Step: 960... Loss: 2.7679... Val Loss: 2.6532\n",
      "Epoch: 2/3... Step: 970... Loss: 2.7333... Val Loss: 2.6480\n",
      "Epoch: 2/3... Step: 980... Loss: 2.6845... Val Loss: 2.6463\n",
      "Epoch: 2/3... Step: 990... Loss: 2.6595... Val Loss: 2.6394\n",
      "Epoch: 2/3... Step: 1000... Loss: 2.6970... Val Loss: 2.6349\n",
      "Epoch: 2/3... Step: 1010... Loss: 2.6789... Val Loss: 2.6284\n",
      "Epoch: 2/3... Step: 1020... Loss: 2.7312... Val Loss: 2.6226\n",
      "Epoch: 2/3... Step: 1030... Loss: 2.6963... Val Loss: 2.6176\n",
      "Epoch: 2/3... Step: 1040... Loss: 2.6762... Val Loss: 2.6133\n",
      "Epoch: 2/3... Step: 1050... Loss: 2.7209... Val Loss: 2.6091\n",
      "Epoch: 2/3... Step: 1060... Loss: 2.7131... Val Loss: 2.6027\n",
      "Epoch: 2/3... Step: 1070... Loss: 2.6737... Val Loss: 2.5984\n",
      "Epoch: 2/3... Step: 1080... Loss: 2.6854... Val Loss: 2.5937\n",
      "Epoch: 2/3... Step: 1090... Loss: 2.6454... Val Loss: 2.5902\n",
      "Epoch: 2/3... Step: 1100... Loss: 2.6791... Val Loss: 2.5851\n",
      "Epoch: 2/3... Step: 1110... Loss: 2.6461... Val Loss: 2.5824\n",
      "Epoch: 3/3... Step: 1120... Loss: 2.6429... Val Loss: 2.5775\n",
      "Epoch: 3/3... Step: 1130... Loss: 2.6230... Val Loss: 2.5740\n",
      "Epoch: 3/3... Step: 1140... Loss: 2.6528... Val Loss: 2.5699\n",
      "Epoch: 3/3... Step: 1150... Loss: 2.6433... Val Loss: 2.5670\n",
      "Epoch: 3/3... Step: 1160... Loss: 2.6779... Val Loss: 2.5631\n",
      "Epoch: 3/3... Step: 1170... Loss: 2.6285... Val Loss: 2.5596\n",
      "Epoch: 3/3... Step: 1180... Loss: 2.6371... Val Loss: 2.5568\n",
      "Epoch: 3/3... Step: 1190... Loss: 2.6427... Val Loss: 2.5531\n",
      "Epoch: 3/3... Step: 1200... Loss: 2.6597... Val Loss: 2.5509\n",
      "Epoch: 3/3... Step: 1210... Loss: 2.6815... Val Loss: 2.5483\n",
      "Epoch: 3/3... Step: 1220... Loss: 2.6686... Val Loss: 2.5454\n",
      "Epoch: 3/3... Step: 1230... Loss: 2.6441... Val Loss: 2.5410\n",
      "Epoch: 3/3... Step: 1240... Loss: 2.6248... Val Loss: 2.5402\n",
      "Epoch: 3/3... Step: 1250... Loss: 2.6974... Val Loss: 2.5366\n",
      "Epoch: 3/3... Step: 1260... Loss: 2.6444... Val Loss: 2.5331\n",
      "Epoch: 3/3... Step: 1270... Loss: 2.6407... Val Loss: 2.5326\n",
      "Epoch: 3/3... Step: 1280... Loss: 2.6230... Val Loss: 2.5285\n",
      "Epoch: 3/3... Step: 1290... Loss: 2.6321... Val Loss: 2.5256\n",
      "Epoch: 3/3... Step: 1300... Loss: 2.5900... Val Loss: 2.5245\n",
      "Epoch: 3/3... Step: 1310... Loss: 2.5882... Val Loss: 2.5226\n",
      "Epoch: 3/3... Step: 1320... Loss: 2.6078... Val Loss: 2.5189\n",
      "Epoch: 3/3... Step: 1330... Loss: 2.6328... Val Loss: 2.5176\n",
      "Epoch: 3/3... Step: 1340... Loss: 2.6176... Val Loss: 2.5162\n",
      "Epoch: 3/3... Step: 1350... Loss: 2.6149... Val Loss: 2.5142\n",
      "Epoch: 3/3... Step: 1360... Loss: 2.6037... Val Loss: 2.5141\n",
      "Epoch: 3/3... Step: 1370... Loss: 2.5919... Val Loss: 2.5110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3... Step: 1380... Loss: 2.5839... Val Loss: 2.5083\n",
      "Epoch: 3/3... Step: 1390... Loss: 2.6429... Val Loss: 2.5065\n",
      "Epoch: 3/3... Step: 1400... Loss: 2.6308... Val Loss: 2.5059\n",
      "Epoch: 3/3... Step: 1410... Loss: 2.5499... Val Loss: 2.5033\n",
      "Epoch: 3/3... Step: 1420... Loss: 2.5558... Val Loss: 2.5015\n",
      "Epoch: 3/3... Step: 1430... Loss: 2.5848... Val Loss: 2.5019\n",
      "Epoch: 3/3... Step: 1440... Loss: 2.5964... Val Loss: 2.4997\n",
      "Epoch: 3/3... Step: 1450... Loss: 2.6019... Val Loss: 2.4980\n",
      "Epoch: 3/3... Step: 1460... Loss: 2.5903... Val Loss: 2.4967\n",
      "Epoch: 3/3... Step: 1470... Loss: 2.6042... Val Loss: 2.4950\n",
      "Epoch: 3/3... Step: 1480... Loss: 2.6425... Val Loss: 2.4931\n",
      "Epoch: 3/3... Step: 1490... Loss: 2.6329... Val Loss: 2.4945\n",
      "Epoch: 3/3... Step: 1500... Loss: 2.5550... Val Loss: 2.4909\n",
      "Epoch: 3/3... Step: 1510... Loss: 2.5854... Val Loss: 2.4891\n",
      "Epoch: 3/3... Step: 1520... Loss: 2.5944... Val Loss: 2.4867\n",
      "Epoch: 3/3... Step: 1530... Loss: 2.5323... Val Loss: 2.4853\n",
      "Epoch: 3/3... Step: 1540... Loss: 2.5695... Val Loss: 2.4858\n",
      "Epoch: 3/3... Step: 1550... Loss: 2.5511... Val Loss: 2.4832\n",
      "Epoch: 3/3... Step: 1560... Loss: 2.6019... Val Loss: 2.4825\n",
      "Epoch: 3/3... Step: 1570... Loss: 2.5778... Val Loss: 2.4804\n",
      "Epoch: 3/3... Step: 1580... Loss: 2.5650... Val Loss: 2.4783\n",
      "Epoch: 3/3... Step: 1590... Loss: 2.5375... Val Loss: 2.4769\n",
      "Epoch: 3/3... Step: 1600... Loss: 2.5672... Val Loss: 2.4777\n",
      "Epoch: 3/3... Step: 1610... Loss: 2.5583... Val Loss: 2.4759\n",
      "Epoch: 3/3... Step: 1620... Loss: 2.5312... Val Loss: 2.4736\n",
      "Epoch: 3/3... Step: 1630... Loss: 2.5841... Val Loss: 2.4719\n",
      "Epoch: 3/3... Step: 1640... Loss: 2.5721... Val Loss: 2.4698\n",
      "Epoch: 3/3... Step: 1650... Loss: 2.5392... Val Loss: 2.4689\n",
      "Epoch: 3/3... Step: 1660... Loss: 2.5002... Val Loss: 2.4683\n",
      "Epoch: 3/3... Step: 1670... Loss: 2.5353... Val Loss: 2.4669\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = 50\n",
    "n_epochs =  3# start small if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "    '''\n",
    "    Given a character, predict the next character\n",
    "    Returns the predicted charater and hidden state\n",
    "    '''\n",
    "    # tensor inputs\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "    \n",
    "    # get the character prob\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        p = p.cpu()\n",
    "        \n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "    \n",
    "    # select the likely next char with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "    \n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming and generating text \n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_x_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said her ween tan tatet ot te siters.\n",
      " Ir wont tar to waris winn herestes on tas ans at annd terand whe and honsses hhildet te ta to ate to, torered wan at tires the hild he wilotit har orere aten ass tind\n",
      "his and\n",
      "she an wosed he andise he watit asd ho her, an whind as wond hes tin she wase hhite thated ther wesann weto at thhe her an wat ans tat,, ad adetes or tores toet, on hind hit han ass hit ass weronde ar ant tat os arter ans the had he sirenn to sant toet\n",
      "ar os and to her ho saters\n",
      "to hin sot astes thos ad toe arese ther ad wosr ot an wint tile whos ande adesann wete went wer hit agilr to site hhe sennn as han tins\n",
      "hos whint\n",
      "hit, the she sent ond tora to ta thhar at ther hee sered to tire an adeted\n",
      "tite te weed hont an sons witatad hhes te wan teet, hils he sotins ore wint to hire, het at to waris ad wene to tite are thit sasses ad har to hhe what ant thor\n",
      "tasers his to ad, hin wilt\n",
      "wond ol to hitite se toe tat thar os te sotit at ol,e. \"Ied wes ot weend this hhas ote te she, the hor te to to soe thin thor were hir tor hond tas\n",
      "atetite tant the atteretis hhinn thee hintede te hond ar waled antentode tan at hor to as, tas or thh thes hot ther an the wols, wen tar hort hers want, tin to tare\n",
      "tas whate ta ho thhit thites asd at hor ard wint os he anntess hhe satinn sind wise te whotid tant to atated on wor te at ated attasese hher to, the,, to an he the attet wal hher hin te tee hir antatitate she\n",
      "totote hontens wero san shins he sato tat at he hes annsres tatann, ad\n",
      "sos tile har here tirand,,\n",
      "shoted hee wols, thon to the and, to wis sals hild.\n",
      "\"\n",
      "I toed wee sans or ot ard to weet wonn so shitiss at hhe ho sititide she he ad os teet west wete we hinn wite ans and and hond whet were and hortet wons\n",
      "ter soesd he hhe sin hhos tar hint, the sotestare os atinn te atann hhan sans to hit hand an went ta sotit and ad torann tans toito ad tisist we shos the thitatter hhins wanne antarind.\n",
      " Anntirtis the and the he than ase arsintes as has on hat hos at, on tas oses, ar woe ans\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
